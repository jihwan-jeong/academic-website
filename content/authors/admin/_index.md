---
title: Jihwan Jeong
role: Ph.D. Candidate at University of Toronto
avatar_filename: avatar.jpg
bio: My research interests include offline reinforcement learning, model-based reinforcement learning, decision-aware model learning, meta-learning and Bayesian deep learning.
interests:
  - Offline reinforcement learning
  - Model-based reinforcement learning
  - Decision-aware model learning
  - Meta-learning
  - Bayesian deep learning
social:
  - icon: envelope
    icon_pack: fas
    link: /#contact
  - display:
      header: false
    link: https://scholar.google.com/citations?user=XvKkcC4AAAAJ&hl=en
    icon_pack: ai
    icon: google-scholar
  - display:
      header: false
    link: https://github.com/jihwan-jeong
    icon_pack: fab
    icon: github
  - icon: linkedin
    icon_pack: fab
    link: https://www.linkedin.com/in/jihwan-jeong-5301ab183/
organizations:
  - name: University of Toronto
    url: https://d3m.mie.utoronto.ca
  - name: Vector Institute
    url: https://vectorinstitute.ai
education:
  courses:
    - course: Ph.D. Candidate in Information Engineering (Present)
      institution: University of Toronto
      year: ""
    - course: M.S. in Industrial and Systems Engineering
      institution: Korea Advanced Institute of Science and Technology (KAIST)
      year: 2019
    - course: B.S. in Chemistry
      institution: Korea Advanced Institute of Science and Technology (KAIST)
      year: 2015
superuser: true
last_name: Jeong
highlight_name: true
first_name: Jihwan
email: jiihwan.jeong@gmail.com
---
Welcome to my profile :smile:! &nbsp; I'm a Ph.D. candidate at the University of Toronto, contributing actively to the D3M (Data-Driven Decision-making) lab under the mentorship of Professor [Scott Sanner](https://d3m.mie.utoronto.ca). My interest in AI and ML is rooted in their potential to revolutionize decision-making in diverse areas. 

My research primarily focuses on leveraging models for enhanced decision-making, with a special emphasis on offline model-based reinforcement learning. This work includes a notable paper accepted at ICLR-23, which explores the use of Bayesian models for robust planning and policy learning by accounting for the epistemic uncertainty of models (learn more [here](project/cbop)).

My internship at Google Research, under the guidance of [Yinlam Chow](https://www.linkedin.com/in/yinlamchow/), was a transformative period. There, I contributed to integrating recommendation systems with large language models (LLMs), applying Reinforcement Learning with AI Feedback (RLAIF) in a novel way to the challenge of recommendation explanations. This culminated in a [first-authored paper](https://arxiv.org/pdf/2310.06176.pdf) that highlights the effective fine-tuning of LLMs for accurate and personalized recommendations. Additionally, I was instrumental in developing the PAX pipeline, a cornerstone for our team's language model projects. (Check out the other paper [here](https://arxiv.org/pdf/2310.04475.pdf)!)

Approaching the completion of my Ph.D., my thesis, tentatively titled "Leveraging Learned Models for Decision-Making," encapsulates my research ethos. It tackles the intricacies of using imperfect models for decision-making by focusing on (1) optimizing decision loss, (2) employing Bayesian methods for uncertainty management, and (4) enabling models and policies to adapt swiftly in new environments.

I look forward to opportunities that will allow me to apply and expand my expertise in AI/ML, aiming to continue making impactful contributions in this dynamic field.

{{< icon name="download" pack="fas" >}} Download my {{< staticref "uploads/cv.pdf" "newtab" >}}CV{{< /staticref >}}.